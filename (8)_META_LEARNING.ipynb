{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMsHKjxOXynFw1HwmSNFvNv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"m9k_GBmmDQ3H"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","source":["# Define a simple task\n","def generate_task():\n","    x = np.random.rand(10)\n","    y = 2 * x + 1\n","    return x, y"],"metadata":{"id":"h6yD0xKpF_rp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize a model with random parameters\n","def initialize_model():\n","    return {'w': np.random.rand(), 'b': np.random.rand()}\n"],"metadata":{"id":"2SRSvQ6kGLdw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the loss function\n","def loss(model, x, y):\n","    y_pred = model['w'] * x + model['b']\n","    return np.mean((y_pred - y)**2)"],"metadata":{"id":"3oblQ58lGOVw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize meta-parameters\n","meta_learning_rate = 0.1\n","num_meta_iterations = 100"],"metadata":{"id":"8nQRYL3XGRsQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Meta-training loop\n","for meta_iteration in range(num_meta_iterations):\n","    task = generate_task()\n","    model = initialize_model()\n","\n","    # Perform task-specific gradient descent\n","    task_learning_rate = 0.01\n","    num_task_iterations = 10\n","\n","    for _ in range(num_task_iterations):\n","        x, y = task\n","        gradients = {}\n","        for param in model:\n","            original_param_value = model[param]\n","            # Calculate the gradient for each parameter\n","            model[param] = original_param_value + task_learning_rate\n","            loss_plus = loss(model, x, y)\n","            model[param] = original_param_value - task_learning_rate\n","            loss_minus = loss(model, x, y)\n","            model[param] = original_param_value\n","            gradients[param] = (loss_plus - loss_minus) / (2 * task_learning_rate)\n","\n","        # Update the model with gradients\n","        for param in model:\n","            model[param] -= meta_learning_rate * gradients[param]\n","\n","    print(f\"Meta-Iteration {meta_iteration+1}: Loss = {loss(model, x, y)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kUvUCrHsGUVp","executionInfo":{"status":"ok","timestamp":1698694872445,"user_tz":420,"elapsed":5,"user":{"displayName":"devarshi modi","userId":"07485293436982757464"}},"outputId":"ba26c592-d128-4c12-e015-823928cf2785"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Meta-Iteration 1: Loss = 0.06879394124047795\n","Meta-Iteration 2: Loss = 0.040383727987053517\n","Meta-Iteration 3: Loss = 0.04330161356056349\n","Meta-Iteration 4: Loss = 0.1410966011190659\n","Meta-Iteration 5: Loss = 0.03972827441468208\n","Meta-Iteration 6: Loss = 0.05771346679054083\n","Meta-Iteration 7: Loss = 0.11893475318572226\n","Meta-Iteration 8: Loss = 0.031113562479306113\n","Meta-Iteration 9: Loss = 0.07247097527296906\n","Meta-Iteration 10: Loss = 0.051047514972944984\n","Meta-Iteration 11: Loss = 0.030825967473825138\n","Meta-Iteration 12: Loss = 0.04286664749723308\n","Meta-Iteration 13: Loss = 0.07571451234310086\n","Meta-Iteration 14: Loss = 0.07796523568019802\n","Meta-Iteration 15: Loss = 0.03874815305833833\n","Meta-Iteration 16: Loss = 0.05612034472976507\n","Meta-Iteration 17: Loss = 0.04511104717807177\n","Meta-Iteration 18: Loss = 0.04657875921928806\n","Meta-Iteration 19: Loss = 0.06365362493164341\n","Meta-Iteration 20: Loss = 0.024598554308151572\n","Meta-Iteration 21: Loss = 0.11141510190034683\n","Meta-Iteration 22: Loss = 0.08206047767917432\n","Meta-Iteration 23: Loss = 0.13805929895784602\n","Meta-Iteration 24: Loss = 0.03611951690914349\n","Meta-Iteration 25: Loss = 0.06006676388952119\n","Meta-Iteration 26: Loss = 0.0496464829664426\n","Meta-Iteration 27: Loss = 0.03414339839612161\n","Meta-Iteration 28: Loss = 0.10056245707075184\n","Meta-Iteration 29: Loss = 0.03086681958876785\n","Meta-Iteration 30: Loss = 0.11571659252744859\n","Meta-Iteration 31: Loss = 0.019646343595459942\n","Meta-Iteration 32: Loss = 0.03689354953506844\n","Meta-Iteration 33: Loss = 0.06152682643587135\n","Meta-Iteration 34: Loss = 0.0634879670569053\n","Meta-Iteration 35: Loss = 0.03592655536677404\n","Meta-Iteration 36: Loss = 0.02555882092197307\n","Meta-Iteration 37: Loss = 0.08472512119659195\n","Meta-Iteration 38: Loss = 0.12189234119548828\n","Meta-Iteration 39: Loss = 0.02677978808065364\n","Meta-Iteration 40: Loss = 0.013326335410538306\n","Meta-Iteration 41: Loss = 0.11317674089461223\n","Meta-Iteration 42: Loss = 0.03779239905000152\n","Meta-Iteration 43: Loss = 0.09843681782975292\n","Meta-Iteration 44: Loss = 0.010439271723188301\n","Meta-Iteration 45: Loss = 0.051556701477278255\n","Meta-Iteration 46: Loss = 0.09262040956836376\n","Meta-Iteration 47: Loss = 0.1170758283207927\n","Meta-Iteration 48: Loss = 0.04702632394576082\n","Meta-Iteration 49: Loss = 0.03542808801103152\n","Meta-Iteration 50: Loss = 0.07041462328868199\n","Meta-Iteration 51: Loss = 0.10543839667682\n","Meta-Iteration 52: Loss = 0.09250551694114093\n","Meta-Iteration 53: Loss = 0.07176933165683208\n","Meta-Iteration 54: Loss = 0.041202599375598635\n","Meta-Iteration 55: Loss = 0.03335847345192949\n","Meta-Iteration 56: Loss = 0.09891513436174185\n","Meta-Iteration 57: Loss = 0.03146749346505971\n","Meta-Iteration 58: Loss = 0.07086676241819516\n","Meta-Iteration 59: Loss = 0.025364747574179086\n","Meta-Iteration 60: Loss = 0.012844203883813039\n","Meta-Iteration 61: Loss = 0.09044271157941887\n","Meta-Iteration 62: Loss = 0.032437720123222866\n","Meta-Iteration 63: Loss = 0.07311373525488626\n","Meta-Iteration 64: Loss = 0.04609537599233694\n","Meta-Iteration 65: Loss = 0.14778085149439363\n","Meta-Iteration 66: Loss = 0.05380585114176715\n","Meta-Iteration 67: Loss = 0.049378180316836204\n","Meta-Iteration 68: Loss = 0.05355726712945948\n","Meta-Iteration 69: Loss = 0.07621706670808241\n","Meta-Iteration 70: Loss = 0.033518598385404705\n","Meta-Iteration 71: Loss = 0.03405387654931179\n","Meta-Iteration 72: Loss = 0.05770551337686255\n","Meta-Iteration 73: Loss = 0.008331776370471809\n","Meta-Iteration 74: Loss = 0.10863267700307958\n","Meta-Iteration 75: Loss = 0.028440979514943638\n","Meta-Iteration 76: Loss = 0.10384378240257215\n","Meta-Iteration 77: Loss = 0.06699021301818327\n","Meta-Iteration 78: Loss = 0.05952395405145635\n","Meta-Iteration 79: Loss = 0.0721772307132967\n","Meta-Iteration 80: Loss = 0.11323865296091976\n","Meta-Iteration 81: Loss = 0.01952522327053947\n","Meta-Iteration 82: Loss = 0.1431072658094732\n","Meta-Iteration 83: Loss = 0.0597009424919019\n","Meta-Iteration 84: Loss = 0.02439468421375724\n","Meta-Iteration 85: Loss = 0.03394015288552804\n","Meta-Iteration 86: Loss = 0.02334466112667247\n","Meta-Iteration 87: Loss = 0.12206519395218471\n","Meta-Iteration 88: Loss = 0.12978334506581396\n","Meta-Iteration 89: Loss = 0.03763422360982692\n","Meta-Iteration 90: Loss = 0.04884575342314496\n","Meta-Iteration 91: Loss = 0.05501919593249717\n","Meta-Iteration 92: Loss = 0.033426015788905764\n","Meta-Iteration 93: Loss = 0.015405998329062764\n","Meta-Iteration 94: Loss = 0.07645069552817936\n","Meta-Iteration 95: Loss = 0.029170359929481228\n","Meta-Iteration 96: Loss = 0.0997667522783347\n","Meta-Iteration 97: Loss = 0.025510718533330968\n","Meta-Iteration 98: Loss = 0.07158445210310595\n","Meta-Iteration 99: Loss = 0.03222206713245417\n","Meta-Iteration 100: Loss = 0.017623018485520305\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zlw8svKQGrc5"},"execution_count":null,"outputs":[]}]}